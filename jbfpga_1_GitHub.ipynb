{"cells":[{"cell_type":"code","source":["# https://github.com/azure/aml-real-time-ai\n"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":1},{"cell_type":"code","source":["!az login\n#!Connect-AzAccount\n"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":2},{"cell_type":"code","source":["!Select-AzSubscription -Subscription \"<YourSubscription>\""],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":3},{"cell_type":"code","source":["import os\nimport tensorflow as tf\nimport numpy as np"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":4},{"cell_type":"code","source":["# Input images as a two-dimensional tensor containing an arbitrary number of images represented a strings\nimport azureml.contrib.brainwave.models.utils as utils\nin_images = tf.placeholder(tf.string)\nimage_tensors = utils.preprocess_array(in_images)\nprint(image_tensors.shape)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">(?, 224, 224, 3)\n</div>"]}}],"execution_count":5},{"cell_type":"code","source":["#Featurizer\n#We use ResNet50 as a featurizer. In this step we initialize the model. This downloads a TensorFlow checkpoint of the quantized ResNet50.\nfrom azureml.contrib.brainwave.models import QuantizedResnet50\nmodel_path = os.path.expanduser('~/models')\nmodel = QuantizedResnet50(model_path, is_frozen = True)\nfeature_tensor = model.import_graph_def(image_tensors)\nprint(model.version)\nprint(feature_tensor.name)\nprint(feature_tensor.shape)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">1.1.2\nresnet_v1_50/pool5_2:0\n(?, 1, 1, 2048)\n</div>"]}}],"execution_count":6},{"cell_type":"code","source":["#Classifier\n#The model we downloaded includes a classifier which takes the output of the ResNet50 and identifies an image. This classifier is trained on the ImageNet dataset. We are going to use this classifier for our service. The next #notebook shows how to train a classifier for a different data set. The input to the classifier is a tensor matching the output of our ResNet50 featurizer.\n\nclassifier_output = model.get_default_classifier(feature_tensor)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":7},{"cell_type":"code","source":["# Service Definition\n#Now that we've definied the image preprocessing, featurizer, and classifier that we will execute on our service we can create a service definition. The service definition is a set of files generated from the model that #allow us to deploy to the FPGA service. The service definition consists of a pipeline. The pipeline is a series of stages that are executed in order. We support TensorFlow stages, Keras stages, and BrainWave stages. The #stages will be executed in order on the service, with the output of each stage input into the subsequent stage.\n#To create a TensorFlow stage we specify a session containing the graph (in this case we are using the default graph) and the input and output tensors to this stage. We use this information to save the graph so that we can #execute it on the service.\n\nfrom azureml.contrib.brainwave.pipeline import ModelDefinition, TensorflowStage, BrainWaveStage\nsave_path = os.path.expanduser('~/models/save')\nmodel_def_path = os.path.join(save_path, 'model_def.zip')\n\nmodel_def = ModelDefinition()\nwith tf.Session() as sess:\n    model_def.pipeline.append(TensorflowStage(sess, in_images, image_tensors))\n    model_def.pipeline.append(BrainWaveStage(sess, model))\n    model_def.pipeline.append(TensorflowStage(sess, feature_tensor, classifier_output))\n    model_def.save(model_def_path)\n    print(model_def_path)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">INFO:tensorflow:Froze 0 variables.\nConverted 0 variables to const ops.\nINFO:tensorflow:Restoring parameters from /root/models/msfprn50/1.1.2/resnet50_bw\nINFO:tensorflow:Froze 0 variables.\nConverted 0 variables to const ops.\n/root/models/save/model_def.zip\n</div>"]}}],"execution_count":8},{"cell_type":"code","source":["# Generate Workspace File for next step\n#import azureml.core\n#print(azureml.core.VERSION)\n\n#from azureml.core import Workspace\n#ws = Workspace.create(name='jbfpgamlws',\n#                      subscription_id='82d7b191-5a8c-4cbf-a9f9-9aa5fb50feaa',\t\n#                      resource_group='jbfpga',\n#                      create_resource_group=False,\n#                      location='eastus2' \n#                     )\n#ws.get_details()\n\n"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":9},{"cell_type":"code","source":["#dbutils.fs.put(\"config.json\", \"{'subscription_id': '82d7b191-5a8c-4cbf-a9f9-9aa5fb50feaa', 'resource_group': 'jbfpga','workspace_name': 'jbfpgamlws'}\")"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":10},{"cell_type":"code","source":["import os\n\nsubscription_id = os.getenv(\"SUBSCRIPTION_ID\", default=\"<YourSubscription ID>\")\nresource_group = os.getenv(\"RESOURCE_GROUP\", default=\"<YourResourcegroup>\")\nworkspace_name = os.getenv(\"WORKSPACE_NAME\", default=\"<YourWorkspace>\")\nworkspace_region = os.getenv(\"WORKSPACE_REGION\", default=\"<YourAzureDCLocation>\")"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":11},{"cell_type":"code","source":["#Access your workspace\nfrom azureml.core import Workspace\n\ntry:\n    ws = Workspace(subscription_id = subscription_id, resource_group = resource_group, workspace_name = workspace_name)\n    # write the details of the workspace to a configuration file to the notebook library\n    ws.write_config()\n    print(\"Workspace configuration succeeded. Skip the workspace creation steps below\")\nexcept:\n    print(\"Workspace not accessible. Change your parameters or create a new workspace below\")"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Wrote the config file config.json to: /databricks/driver/aml_config/config.json\nWorkspace configuration succeeded. Skip the workspace creation steps below\n</div>"]}}],"execution_count":12},{"cell_type":"code","source":["#Deploy\n#Time to create a service from the service definition. You need a Workspace in the East US 2 location. In the previous notebooks, you've created this Workspace. The code below will load that Workspace from a configuration #file.\nfrom azureml.core import Workspace\n\nws = Workspace.from_config()\nprint(ws.name, ws.resource_group, ws.location, ws.subscription_id, sep = '\\n')\n"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Found the config file in: /databricks/driver/aml_config/config.json\njbfpgamlws\njbfpga\neastus2\n82d7b191-5a8c-4cbf-a9f9-9aa5fb50feaa\n</div>"]}}],"execution_count":13},{"cell_type":"code","source":["#Upload the model to the workspace.\nfrom azureml.core.model import Model\n\nmodel_name = \"resnet-50-rtai\"\n#model_def_path = \"/databricks/driver/aml_config/\"\n#model_def_path = os.path.expanduser('~/')\nsave_path = os.path.expanduser('~/models/save')\nmodel_def_path = os.path.join(save_path, 'model_def.zip')\n\nregistered_model = Model.register(ws, model_def_path, model_name)\n\n"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Registering model resnet-50-rtai\n</div>"]}}],"execution_count":14},{"cell_type":"code","source":["from azureml.core import Workspace\n\nws = Workspace.from_config()\nprint(ws.name, ws.resource_group, ws.location, ws.subscription_id, sep = '\\n')\n\nfrom azureml.core.model import Model\nmodel_name = \"resnet-50-rtai\"\nsave_path = os.path.expanduser('~/models/save')\nmodel_def_path = os.path.join(save_path, 'model_def.zip')\nregistered_model = Model.register(ws, model_def_path, model_name)\n\nfrom azureml.core.webservice import Webservice\nfrom azureml.exceptions import WebserviceException\nfrom azureml.contrib.brainwave import BrainwaveWebservice, BrainwaveImage\nservice_name = \"imagenet-infer\"\nservice = None\ntry:\n    service = Webservice(ws, service_name)\nexcept WebserviceException:\n    image_config = BrainwaveImage.image_configuration()\n    deployment_config = BrainwaveWebservice.deploy_configuration()\n    \n    print(ws, service_name,[registered_model],image_config,deployment_config, sep = '\\n')\n    \n    service = Webservice.deploy_from_model(ws, service_name, [registered_model], image_config, deployment_config)\n    service.wait_for_deployment(True)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Found the config file in: /databricks/driver/aml_config/config.json\njbfpgamlws\njbfpga\neastus2\n82d7b191-5a8c-4cbf-a9f9-9aa5fb50feaa\nRegistering model resnet-50-rtai\n&lt;azureml.core.workspace.Workspace object at 0x7f86fb54aa20&gt;\nimagenet-infer\n[&lt;azureml.core.model.Model object at 0x7f86fb54ac50&gt;]\n&lt;azureml.contrib.brainwave.brainwave_image.BrainwaveImageConfiguration object at 0x7f8702486780&gt;\n&lt;azureml.contrib.brainwave.brainwave_webservice.RealTimeAIWebserviceDeploymentConfiguration object at 0x7f87306d2c18&gt;\nCreating image\nImage creation operation finished for image imagenet-infer:19, operation &quot;Succeeded&quot;\nCreating service\nRunning....................................\nSucceededFPGA service creation operation finished, operation &quot;Succeeded&quot;\n</div>"]}}],"execution_count":15},{"cell_type":"code","source":["#Client\n#The service supports gRPC and the TensorFlow Serving \"predict\" API. We provide a client that can call the service to get predictions on aka.ms/rtai. You can also invoke the service like any other web service.\n#\n#To understand the results we need a mapping to the human readable imagenet classes\nimport requests\nclasses_entries = requests.get(\"https://raw.githubusercontent.com/Lasagne/Recipes/master/examples/resnet50/imagenet_classes.txt\").text.splitlines()\n\nfrom azure.storage.blob import BlockBlobService\nblock_blob_service = BlockBlobService(account_name='<YourStorageAcctName>', account_key='<YourStorageAcctKey>')\nblock_blob_service.get_blob_to_path('images', 'Snowleopard.jpg', 'TestImage.jpg')\n\n#https://docs.microsoft.com/en-us/azure/machine-learning/service/how-to-consume-web-service\n\nprint(service.scoring_uri)\nprimary, secondary = service.get_keys()\nprint(primary)\nprint(secondary)\n\n#service = Webservice(workspace=ws, name='imagenet-infer')\nprint(service.scoring_uri)\n\n# We can now send an image to the service and get the predictions. Let's see if it can identify a snow leopard. \nresults = service.run('TestImage.jpg')\n# map results [class_id] => [confidence]\nresults = enumerate(results)\n# sort results by confidence\nsorted_results = sorted(results, key=lambda x: x[1], reverse=True)\n# print top 5 results\nfor top in sorted_results[:5]:\n    print(classes_entries[top[0]], 'confidence:', top[1])"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">168.62.38.171:80\nQ20U5fgJh99gVVcjFDprM1FHvGuxuUSE\n51sYQX2PFbjaXuyg8nyqBhZsFjrnqiGC\n168.62.38.171:80\nsnow leopard, ounce, Panthera uncia confidence: 0.9519317\nleopard, Panthera pardus confidence: 0.047500856\njaguar, panther, Panthera onca, Felis onca confidence: 0.00023210353\ncheetah, chetah, Acinonyx jubatus confidence: 0.00017502057\nlynx, catamount confidence: 7.8550125e-05\n</div>"]}}],"execution_count":16}],"metadata":{"name":"jbfpga_1_GitHub","notebookId":2660012586642073},"nbformat":4,"nbformat_minor":0}
