{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/Azure/aml-real-time-ai/blob/600c67304fdffd92694ab9551fa7b8be9628d47c/notebooks/project-brainwave-transfer-learning.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "C:\\Anaconda\\lib\\site-packages\\requests\\__init__.py:91: RequestsDependencyWarning: urllib3 (1.25.1) or chardet (3.0.4) doesn't match a supported version!\n",
      "  RequestsDependencyWarning)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install --upgrade azureml-sdk[contrib]\n",
    "#!pip install --upgrade azureml-sdk[contrib,notebooks,explain,automl,databricks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install \"tensorflow==1.10\" --force\n",
    "#!pip install --upgrade tensorflow --force"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\JBAdmin/catsanddogs\\PetImages\\Cat\\0.jpg\n",
      "C:\\Users\\JBAdmin/catsanddogs\\PetImages\\Dog\\0.jpg\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import imghdr\n",
    "datadir = os.path.expanduser(\"~/catsanddogs\")\n",
    "\n",
    "cat_files = glob.glob(os.path.join(datadir, 'PetImages', 'Cat', '*.jpg'))\n",
    "dog_files = glob.glob(os.path.join(datadir, 'PetImages', 'Dog', '*.jpg'))\n",
    "\n",
    "# Limit the data set to make the notebook execute quickly.\n",
    "cat_files = cat_files[:128]\n",
    "dog_files = dog_files[:128]\n",
    "\n",
    "# The data set has a few images that are not jpeg. Remove them.\n",
    "cat_files = [f for f in cat_files if imghdr.what(f) == 'jpeg']\n",
    "dog_files = [f for f in dog_files if imghdr.what(f) == 'jpeg']\n",
    "\n",
    "if(not len(cat_files) or not len(dog_files)):\n",
    "    print(\"Please download the Kaggle Cats and Dogs dataset form https://www.microsoft.com/en-us/download/details.aspx?id=54765 and extract the zip to \" + datadir)    \n",
    "    raise ValueError(\"Data not found\")\n",
    "else:\n",
    "    print(cat_files[0])\n",
    "    print(dog_files[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# constructing a numpy array as labels\n",
    "image_paths = cat_files + dog_files\n",
    "total_files = len(cat_files) + len(dog_files)\n",
    "labels = np.zeros(total_files)\n",
    "labels[len(cat_files):] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to preprocess the input file to get it into the form expected by ResNet152. We've provided a default implementation of the preprocessing that you can use.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install --upgrade azureml-sdk[accel]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: \"azureml.contrib.brainwave\" will be deprecated in a future release,please use \"azureml.accel.models\" instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 224, 224, 3)\n"
     ]
    }
   ],
   "source": [
    "# Input images as a two-dimensional tensor containing an arbitrary number of images represented a strings\n",
    "import azureml.contrib.brainwave.models.utils as utils\n",
    "#import azureml.accel.models.utils as utils\n",
    "in_images = tf.placeholder(tf.string)\n",
    "image_tensors = utils.preprocess_array(in_images)\n",
    "print(image_tensors.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.1.3\n"
     ]
    }
   ],
   "source": [
    "from azureml.contrib.brainwave.models import QuantizedResnet152\n",
    "model_path = os.path.expanduser('~/models')\n",
    "bwmodel = QuantizedResnet152(model_path, is_frozen = True)\n",
    "print(bwmodel.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = bwmodel.import_graph_def(input_tensor=image_tensors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-Compute features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "51it [07:30,  7.71s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(255, 1, 1, 2048)\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def chunks(l, n):\n",
    "    \"\"\"Yield successive n-sized chunks from l.\"\"\"\n",
    "    for i in range(0, len(l), n):\n",
    "        yield l[i:i + n]\n",
    "\n",
    "def read_files(files):\n",
    "    contents = []\n",
    "    for path in files:\n",
    "        with open(path, 'rb') as f:\n",
    "            contents.append(f.read())\n",
    "    return contents\n",
    "        \n",
    "feature_list = []\n",
    "with tf.Session() as sess:\n",
    "    for chunk in tqdm(chunks(image_paths, 5)):\n",
    "        contents = read_files(chunk)\n",
    "        result = sess.run([features], feed_dict={in_images: contents})\n",
    "        feature_list.extend(result[0])\n",
    "\n",
    "feature_results = np.array(feature_list)\n",
    "print(feature_results.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add and Train the classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dropout, Dense, Flatten\n",
    "from keras import optimizers\n",
    "\n",
    "FC_SIZE = 1024\n",
    "NUM_CLASSES = 2\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dropout(0.2, input_shape=(1, 1, 2048,)))\n",
    "model.add(Dense(FC_SIZE, activation='relu', input_dim=(1, 1, 2048,)))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(NUM_CLASSES, activation='sigmoid', input_dim=FC_SIZE))\n",
    "\n",
    "model.compile(optimizer=optimizers.SGD(lr=1e-4,momentum=0.9), loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the train and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(191, 1, 1, 2048) (64, 1, 1, 2048) (191, 2) (64, 2)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "onehot_labels = np.array([[0,1] if i else [1,0] for i in labels])\n",
    "X_train, X_test, y_train, y_test = train_test_split(feature_results, onehot_labels, random_state=42, shuffle=True)\n",
    "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/16\n",
      "191/191 [==============================] - ETA: 27s - loss: 0.7913 - acc: 0.50 - ETA: 2s - loss: 0.7563 - acc: 0.5430 - 6s 30ms/step - loss: 0.7604 - acc: 0.5183\n",
      "Epoch 2/16\n",
      "191/191 [==============================] - ETA: 0s - loss: 0.7781 - acc: 0.484 - ETA: 0s - loss: 0.7665 - acc: 0.484 - 0s 764us/step - loss: 0.7618 - acc: 0.4791\n",
      "Epoch 3/16\n",
      "191/191 [==============================] - ETA: 0s - loss: 0.7532 - acc: 0.515 - ETA: 0s - loss: 0.7361 - acc: 0.520 - ETA: 0s - loss: 0.7467 - acc: 0.490 - 0s 853us/step - loss: 0.7361 - acc: 0.4948\n",
      "Epoch 4/16\n",
      "191/191 [==============================] - ETA: 0s - loss: 0.6708 - acc: 0.656 - ETA: 0s - loss: 0.7141 - acc: 0.510 - 0s 780us/step - loss: 0.7117 - acc: 0.5209\n",
      "Epoch 5/16\n",
      "191/191 [==============================] - ETA: 0s - loss: 0.7126 - acc: 0.484 - ETA: 0s - loss: 0.7135 - acc: 0.510 - 0s 783us/step - loss: 0.7101 - acc: 0.5340\n",
      "Epoch 6/16\n",
      "191/191 [==============================] - ETA: 0s - loss: 0.7139 - acc: 0.468 - ETA: 0s - loss: 0.7073 - acc: 0.519 - 0s 676us/step - loss: 0.7034 - acc: 0.5366\n",
      "Epoch 7/16\n",
      "191/191 [==============================] - ETA: 0s - loss: 0.6768 - acc: 0.468 - ETA: 0s - loss: 0.6853 - acc: 0.527 - 0s 664us/step - loss: 0.6997 - acc: 0.5393\n",
      "Epoch 8/16\n",
      "191/191 [==============================] - ETA: 0s - loss: 0.6932 - acc: 0.578 - ETA: 0s - loss: 0.7094 - acc: 0.546 - 0s 667us/step - loss: 0.7098 - acc: 0.5236\n",
      "Epoch 9/16\n",
      "191/191 [==============================] - ETA: 0s - loss: 0.6887 - acc: 0.546 - ETA: 0s - loss: 0.6949 - acc: 0.536 - 0s 739us/step - loss: 0.6904 - acc: 0.5497\n",
      "Epoch 10/16\n",
      "191/191 [==============================] - ETA: 0s - loss: 0.6928 - acc: 0.453 - ETA: 0s - loss: 0.6916 - acc: 0.519 - 0s 798us/step - loss: 0.6907 - acc: 0.5236\n",
      "Epoch 11/16\n",
      "191/191 [==============================] - ETA: 0s - loss: 0.6476 - acc: 0.578 - ETA: 0s - loss: 0.7014 - acc: 0.503 - 0s 725us/step - loss: 0.6975 - acc: 0.5183\n",
      "Epoch 12/16\n",
      "191/191 [==============================] - ETA: 0s - loss: 0.6500 - acc: 0.625 - ETA: 0s - loss: 0.6882 - acc: 0.536 - ETA: 0s - loss: 0.6756 - acc: 0.562 - 0s 867us/step - loss: 0.6823 - acc: 0.5654\n",
      "Epoch 13/16\n",
      "191/191 [==============================] - ETA: 0s - loss: 0.7092 - acc: 0.546 - ETA: 0s - loss: 0.6700 - acc: 0.609 - ETA: 0s - loss: 0.6714 - acc: 0.600 - 0s 874us/step - loss: 0.6771 - acc: 0.5942\n",
      "Epoch 14/16\n",
      "191/191 [==============================] - ETA: 0s - loss: 0.6949 - acc: 0.515 - ETA: 0s - loss: 0.6815 - acc: 0.567 - ETA: 0s - loss: 0.6779 - acc: 0.584 - 0s 885us/step - loss: 0.6775 - acc: 0.5916\n",
      "Epoch 15/16\n",
      "191/191 [==============================] - ETA: 0s - loss: 0.6905 - acc: 0.468 - ETA: 0s - loss: 0.6752 - acc: 0.546 - ETA: 0s - loss: 0.6785 - acc: 0.550 - 0s 880us/step - loss: 0.6883 - acc: 0.5340\n",
      "Epoch 16/16\n",
      "191/191 [==============================] - ETA: 0s - loss: 0.6871 - acc: 0.515 - ETA: 0s - loss: 0.6724 - acc: 0.557 - ETA: 0s - loss: 0.6687 - acc: 0.556 - 0s 851us/step - loss: 0.6761 - acc: 0.5497\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x22cb3434c88>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, epochs=16, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 1 0 1 0 0 0 0 1 0 1 0 0 0 1 0 0 1 0 0 0 1 1 0 0 1 0 0 1 1 1 1 1 1 1 0\n",
      " 1 0 1 0 0 1 0 1 0 0 0 1 0 0 0 1 0 1 1 1 0 0 0 1 1 0 1]\n",
      "[1 0 0 1 0 1 1 1 0 0 1 1 1 1 1 0 1 0 0 0 1 0 0 1 0 0 1 1 0 1 1 0 1 1 1 0 0\n",
      " 0 0 1 1 1 1 1 0 0 0 0 1 1 1 0 1 0 0 1 0 1 1 0 0 1 1 0]\n"
     ]
    }
   ],
   "source": [
    "from numpy import argmax\n",
    "\n",
    "y_probs = model.predict(X_test)\n",
    "y_prob_max = np.argmax(y_probs, 1)\n",
    "y_test_max = np.argmax(y_test, 1)\n",
    "print(y_prob_max)\n",
    "print(y_test_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy 0.46875\n",
      "Precision 0.5\n",
      "Recall 0.4117647058823529\n",
      "F1 0.45161290322580644\n",
      "AUC 0.5362745098039216\n",
      "Confusion Matrix [[16, 14], [20, 14]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, roc_auc_score, accuracy_score, precision_score, recall_score, f1_score\n",
    "import itertools\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# compute a bunch of classification metrics \n",
    "def classification_metrics(y_true, y_pred, y_prob):\n",
    "    cm_dict = {}\n",
    "    cm_dict['Accuracy'] = accuracy_score(y_true, y_pred)\n",
    "    cm_dict['Precision'] =  precision_score(y_true, y_pred)\n",
    "    cm_dict['Recall'] =  recall_score(y_true, y_pred)\n",
    "    cm_dict['F1'] =  f1_score(y_true, y_pred) \n",
    "    cm_dict['AUC'] = roc_auc_score(y_true, y_prob[:,0])\n",
    "    cm_dict['Confusion Matrix'] = confusion_matrix(y_true, y_pred).tolist()\n",
    "    return cm_dict\n",
    "\n",
    "def plot_confusion_matrix(cm, classes, normalize=False, title='Confusion matrix', cmap=plt.cm.Blues):\n",
    "    \"\"\"Plots a confusion matrix.\n",
    "    Source: http://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html\n",
    "    New BSD License - see appendix\n",
    "    \"\"\"\n",
    "    cm_max = cm.max()\n",
    "    cm_min = cm.min()\n",
    "    if cm_min > 0: cm_min = 0\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        cm_max = 1\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "    thresh = cm_max / 2.\n",
    "    plt.clim(cm_min, cm_max)\n",
    "\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i,\n",
    "                 round(cm[i, j], 3),  # round to 3 decimals if they are float\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.show()\n",
    "    \n",
    "cm_dict = classification_metrics(y_test_max, y_prob_max, y_probs)\n",
    "for m in cm_dict:\n",
    "    print(m, cm_dict[m])\n",
    "cm = np.asarray(cm_dict['Confusion Matrix'])\n",
    "plot_confusion_matrix(cm, ['fail','pass'], normalize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Service Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install \"tensorflow==1.10\" --force\n",
    "#!pip uninstall tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install \"tensorflow==1.10\" --force\n",
    "#!pip install --upgrade tensorflow --force"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Froze 0 variables.\n",
      "Converted 0 variables to const ops.\n",
      "INFO:tensorflow:Restoring parameters from C:\\Users\\JBAdmin/models\\msfprn152\\1.1.3\\resnet152_bw\n",
      "INFO:tensorflow:Froze 4 variables.\n",
      "Converted 4 variables to const ops.\n",
      "C:\\Users\\JBAdmin/catsanddogs\\save\\model_def.zip\n"
     ]
    }
   ],
   "source": [
    "from azureml.contrib.brainwave.pipeline import ModelDefinition, TensorflowStage, BrainWaveStage, KerasStage\n",
    "\n",
    "model_def = ModelDefinition()\n",
    "model_def.pipeline.append(TensorflowStage(tf.Session(), in_images, image_tensors))\n",
    "model_def.pipeline.append(BrainWaveStage(tf.Session(), bwmodel))\n",
    "model_def.pipeline.append(KerasStage(model))\n",
    "\n",
    "model_def_path = os.path.join(datadir, 'save', 'model_def.zip')\n",
    "model_def.save(model_def_path)\n",
    "print(model_def_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deploy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jbfpgamlws\n",
      "jbfpga\n",
      "eastus2\n",
      "82d7b191-5a8c-4cbf-a9f9-9aa5fb50feaa\n",
      "Registering model catsanddogs-model\n"
     ]
    }
   ],
   "source": [
    "from azureml.core.model import Model\n",
    "from azureml.core import Workspace\n",
    "\n",
    "ws = Workspace.from_config()\n",
    "print(ws.name, ws.resource_group, ws.location, ws.subscription_id, sep = '\\n')\n",
    "model_name = \"catsanddogs-model\"\n",
    "service_name = \"modelbuild-service\"\n",
    "\n",
    "registered_model = Model.register(ws, model_def_path, model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating image\n",
      "Image creation operation finished for image modelbuild-service:3, operation \"Succeeded\"\n",
      "Creating service\n",
      "Running........................................................................\n",
      "SucceededFPGA service creation operation finished, operation \"Succeeded\"\n"
     ]
    }
   ],
   "source": [
    "from azureml.core.webservice import Webservice\n",
    "from azureml.exceptions import WebserviceException\n",
    "from azureml.contrib.brainwave import BrainwaveWebservice, BrainwaveImage\n",
    "try:\n",
    "    service = Webservice(ws, service_name)\n",
    "except WebserviceException:\n",
    "    image_config = BrainwaveImage.image_configuration()\n",
    "    deployment_config = BrainwaveWebservice.deploy_configuration()\n",
    "    service = Webservice.deploy_from_model(ws, service_name, [registered_model], image_config, deployment_config)\n",
    "    service.wait_for_deployment(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The service is now running in Azure and ready to serve requests. We can check the address and port."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52.170.115.110:80\n"
     ]
    }
   ],
   "source": [
    "print(service.ip_address + ':' + str(service.port))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.contrib.brainwave.client import PredictionClient\n",
    "def run(ip_address, port, input_data):\n",
    "    client = PredictionClient(service.ipAddress, service.port)\n",
    "    #client = PredictionClient(ip_address, self.port, False, \"\")\n",
    "    #if isinstance(input_data, str):\n",
    "    #   return client.score_image(input_data)\n",
    "    #if isinstance(input_data, np.ndarray):\n",
    "    #   return client.score_numpy_array(input_data)\n",
    "    #return client.score_file(input_data.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install azureml-contrib-brainwave\n",
    "#!pip install git+https://github.com/Azure/aml-real-time-ai/tree/master/pythonlib\n",
    "#!pip install git+https://github.com/Azure/aml-real-time-ai/tree/master/pythonlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# #import azureml.accel.models.utils as utils\n",
    "#from azureml.contrib.brainwave.client import PredictionClient\n",
    "from azureml.contrib.brainwave.client import PredictionClient\n",
    "#from amlrealtimeai import PredictionClient # for sample classes in GitHub\n",
    "#from aml-real-time-ai import PredictionClient # for sample classes in GitHub\n",
    "#from azureml.contrib.brainwave.client import PredictionClient\n",
    "#print(PredictionClient)\n",
    "client = PredictionClient(service.ip_address, service.port)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CATS\n",
      "CORRECT [0.73331517 0.6330011 ]\n",
      "WRONG [0.3402378 0.513855 ]\n",
      "CORRECT [0.4637577  0.40981242]\n",
      "WRONG [0.3930712 0.5067719]\n",
      "CORRECT [0.576025   0.36346677]\n",
      "WRONG [0.41783664 0.54699445]\n",
      "WRONG [0.41781354 0.4930551 ]\n",
      "CORRECT [0.47046134 0.37351555]\n",
      "DOGS\n",
      "WRONG [0.64932466 0.6356455 ]\n",
      "WRONG [0.70160306 0.5213355 ]\n",
      "CORRECT [0.46283093 0.5326308 ]\n",
      "WRONG [0.5426064  0.32584208]\n",
      "CORRECT [0.39951482 0.6635674 ]\n",
      "CORRECT [0.4432078  0.44594315]\n",
      "CORRECT [0.7041569 0.715247 ]\n",
      "CORRECT [0.3480113  0.73805165]\n"
     ]
    }
   ],
   "source": [
    "# Specify an image to classify\n",
    "print('CATS')\n",
    "for image_file in cat_files[:8]:\n",
    "    results = client.score_image(image_file)\n",
    "    result = 'CORRECT ' if results[0] > results[1] else 'WRONG '\n",
    "    print(result + str(results))\n",
    "print('/nDOGS')\n",
    "for image_file in dog_files[:8]:\n",
    "    results = client.score_image(image_file)\n",
    "    result = 'CORRECT ' if results[1] > results[0] else 'WRONG '\n",
    "    print(result + str(results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
